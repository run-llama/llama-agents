services:
  message_queue:
    image: llamaindex/llama-deploy:v0.1.3
    environment:
      SIMPLE_MESSAGE_QUEUE_HOST: message_queue
      SIMPLE_MESSAGE_QUEUE_PORT: 8001
      SIMPLE_MESSAGE_QUEUE_INTERNAL_HOST: 0.0.0.0
      SIMPLE_MESSAGE_QUEUE_INTERNAL_PORT: 8001
      MESSAGE_QUEUE_CONFIG: simple
      RUN_CONTROL_PLANE: false
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8001/"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s

  control_plane:
    image: llamaindex/llama-deploy:v0.1.3
    environment:
      CONTROL_PLANE_HOST: control_plane
      CONTROL_PLANE_PORT: 8000
      CONTROL_PLANE_INTERNAL_HOST: 0.0.0.0
      CONTROL_PLANE_INTERNAL_PORT: 8000
      SIMPLE_MESSAGE_QUEUE_HOST: message_queue
      SIMPLE_MESSAGE_QUEUE_PORT: 8001
      MESSAGE_QUEUE_CONFIG: simple
      RUN_MESSAGE_QUEUE: false
    ports:
      - "8000:8000"
    depends_on:
      message_queue:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8000/"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s
