version: "3.8"

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage

  message_queue:
    build:
      context: ./llama_deploy_app/message_queue
      dockerfile: dockerfile
    environment:
      SIMPLE_MESSAGE_QUEUE_HOST: message_queue
      SIMPLE_MESSAGE_QUEUE_PORT: 8001
      SIMPLE_MESSAGE_QUEUE_INTERNAL_HOST: 0.0.0.0
      SIMPLE_MESSAGE_QUEUE_INTERNAL_PORT: 8001
    ports:
      - "8001:8001"
    volumes:
      - ./llama_deploy_app/message_queue:/app
    depends_on:
      - qdrant
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8001/"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s

  control_plane:
    build:
      context: ./llama_deploy_app/control_plane
      dockerfile: dockerfile
    environment:
      CONTROL_PLANE_HOST: control_plane
      CONTROL_PLANE_PORT: 8000
      CONTROL_PLANE_INTERNAL_HOST: 0.0.0.0
      CONTROL_PLANE_INTERNAL_PORT: 8000
      SIMPLE_MESSAGE_QUEUE_HOST: message_queue
      SIMPLE_MESSAGE_QUEUE_PORT: 8001
    ports:
      - "8000:8000"
    volumes:
      - ./llama_deploy_app/control_plane:/app
    depends_on:
      message_queue:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8000/"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s

  rag_workflow:
    build:
      context: ./llama_deploy_app/workflows
      dockerfile: dockerfile
    ports:
      - "8002:8002"
    volumes:
      - ./llama_deploy_app/workflows:/app
    depends_on:
      control_plane:
        condition: service_started
    environment:
      DEPLOY_SETTINGS_NAME: rag_workflow
      OPENAI_API_KEY: $OPENAI_API_KEY
      CONTROL_PLANE_HOST: control_plane
      CONTROL_PLANE_PORT: 8000
    deploy:
      replicas: 1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8002/"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s

  agentic_workflow:
    build:
      context: ./llama_deploy_app/workflows
      dockerfile: dockerfile
    ports:
      - "8003:8003"
    volumes:
      - ./llama_deploy_app/workflows:/app
    depends_on:
      control_plane:
        condition: service_started
    environment:
      DEPLOY_SETTINGS_NAME: agentic_workflow
      OPENAI_API_KEY: $OPENAI_API_KEY
      CONTROL_PLANE_HOST: control_plane
      CONTROL_PLANE_PORT: 8000
    deploy:
      replicas: 1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8003/"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s

  frontend:
    build:
      context: ./llama_deploy_app/frontend
      dockerfile: dockerfile
    environment:
      CONTROL_PLANE_HOST: control_plane
      CONTROL_PLANE_PORT: 8000
    ports:
      - "3000:3000"
      - "9000:8000"
    volumes:
      - ./llama_deploy_app/frontend:/app
    depends_on:
      rag_workflow:
        condition: service_started
      agentic_workflow:
        condition: service_started

volumes:
  qdrant_data:
