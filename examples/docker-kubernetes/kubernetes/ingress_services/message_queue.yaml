---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: message-queue
  namespace: llama-agents-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: message-queue
  template:
    metadata:
      labels:
        app: message-queue
    spec:
      containers:
        - name: message-queue
          env:
            - name: MESSAGE_QUEUE_HOST
              valueFrom:
                configMapKeyRef:
                  name: xcore-config
                  key: MESSAGE_QUEUE_HOST
            - name: MESSAGE_QUEUE_PORT
              valueFrom:
                configMapKeyRef:
                  name: xcore-config
                  key: MESSAGE_QUEUE_PORT
          image: multi_agent_app:latest
          imagePullPolicy: Never
          command:
            [
              "uvicorn",
              "multi_agent_app.core_services.message_queue:app",
              "--host",
              "0.0.0.0",
              "--port",
              "8000",
              "--log-config",
              "./logging.ini",
              "--log-level",
              "debug",
            ]
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          ports:
            - containerPort: 8000

---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: message-queue
  name: message-queue
  namespace: llama-agents-demo
spec:
  selector:
    app: message-queue
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: message-queue
  namespace: llama-agents-demo
spec:
  rules:
    - host: message-queue.127.0.0.1.nip.io
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: message-queue
                port:
                  number: 8000
