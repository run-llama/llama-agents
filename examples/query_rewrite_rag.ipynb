{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49814a62-0201-4d41-a449-c2c6b4ad2ced",
   "metadata": {},
   "source": [
    "# Query Rewriting RAG using Llama Agents\n",
    "\n",
    "In this notebook, we setup two services from query components: \n",
    "\n",
    "1. A query rewriting service\n",
    "2. a RAG service \n",
    "\n",
    "Both of these services will be chained together in a simple constrained flow using our Pipeline Orchestrator.\n",
    "\n",
    "After testing our `llama-agents` system, we then detail how to deploy it as a local set of servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcdf2f6-a3c4-49cd-a61f-f7a682c75e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfccb47-23c5-4496-91af-abc00e7978f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\"\n",
    "\n",
    "## Load Data\n",
    "\n",
    "First, we load our data and parse it with LlamaParse.\n",
    "\n",
    "If you don't have an API key, you can get one for free at [https://cloud.llamaindex.ai](https://cloud.llamaindex.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa71824",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/10k/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe13aa4-55f7-4854-b2b7-b3ae02d8df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id cac11eca-f530-4251-a839-f528bb42b029\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(result_type=\"text\")\n",
    "docs = parser.load_data(\"data/10k/uber_2021.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968eeda",
   "metadata": {},
   "source": [
    "Next, we index are data and cache to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9feb7-bbd9-46cd-8415-335d37d83827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"storage\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed9ff6-7195-4213-b8a1-38dc06c3b25d",
   "metadata": {},
   "source": [
    "## Setup Agents\n",
    "\n",
    "We define a few custom agents: \n",
    "- a retriever agent that will return nodes based on a custom query string\n",
    "- a query rewrite agent that rewrites using a HyDE prompt\n",
    "\n",
    "The agents are defined using the `FnAgentWorker` -- the requirement here is to pass in a function that takes a state dict, performs some operation, and returns the modified state and a boolean indicating if another reasoning loop is needed.\n",
    "\n",
    "The state has two special keys:\n",
    "- `__task__` -- this contains the original input to the agent\n",
    "- `__output__` -- once `is_done=True`, the output should hold the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3a43e-a50b-499b-b94a-91324866716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define router agent\n",
    "\n",
    "# from llama_index.core.agent import FnAgentWorker\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.query_pipeline import QueryPipeline, FnComponent, Link\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "OPENAI_LLM = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# use HyDE to hallucinate answer.\n",
    "HYDE_PROMPT_STR = (\n",
    "    \"Please write a passage to answer the question\\n\"\n",
    "    \"Try to include as many key details as possible.\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    'Passage:\"\"\"\\n'\n",
    ")\n",
    "HYDE_PROMPT_TMPL = PromptTemplate(HYDE_PROMPT_STR)\n",
    "\n",
    "\n",
    "def run_hdye(input_str: str) -> str:\n",
    "    \"\"\"Run HyDE prompt.\"\"\"\n",
    "    qp = QueryPipeline(chain=[HYDE_PROMPT_TMPL, OPENAI_LLM])\n",
    "    output = qp.run(query_str=input_str)\n",
    "    return str(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f895e-52b9-4fd8-b928-6aec19ba000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_component = FnComponent(fn=run_hdye)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c19f5",
   "metadata": {},
   "source": [
    "Next, we define a similar agent to perform RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc290c-1650-480f-8cb7-b81a9f1e1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RAG agent\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "llm = OPENAI_LLM\n",
    "summarizer = TreeSummarize(llm=llm)\n",
    "\n",
    "\n",
    "def run_rag_fn(hyde_answer_str: str, input_str: str) -> str:\n",
    "    \"\"\"Run RAG.\"\"\"\n",
    "    retrieved_nodes = retriever.retrieve(hyde_answer_str)\n",
    "    response = summarizer.synthesize(input_str, retrieved_nodes)\n",
    "    return str(response)\n",
    "\n",
    "\n",
    "rag_component = FnComponent(fn=run_rag_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c488181-3780-429d-b00d-291584b04232",
   "metadata": {},
   "source": [
    "## Setup Agent Services\n",
    "\n",
    "Now, we are ready to build our `llama-agents` system. This includes\n",
    "- A `AgentService` for each agent\n",
    "- A `PipelineOrchestrator` defining the logic for defining the overall flow of tasks through the system\n",
    "- A `SimpleMessageQueue` to facilitate message passing and communcation\n",
    "- A `ControlPlaneServer` to act as the main control-plane for the system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eccd16-4724-4f05-b811-0bd8a284451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    PipelineOrchestrator,\n",
    "    ServiceComponent,\n",
    "    ComponentService,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.query_pipeline import Link, InputComponent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "message_queue = SimpleMessageQueue()\n",
    "\n",
    "## Define Agent Services\n",
    "query_rewrite_server = ComponentService(\n",
    "    component=hyde_component,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Used to rewrite queries\",\n",
    "    service_name=\"query_rewrite_component\",\n",
    ")\n",
    "\n",
    "query_rewrite_server_c = ServiceComponent.from_component_service(query_rewrite_server)\n",
    "\n",
    "rag_server = ComponentService(\n",
    "    component=rag_component, message_queue=message_queue, description=\"rag_agent\"\n",
    ")\n",
    "rag_server_c = ServiceComponent.from_component_service(rag_server)\n",
    "\n",
    "# TODO: make more seamless from local\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    module_dict={\n",
    "        \"input\": InputComponent(),\n",
    "        \"query_rewrite_server_c\": query_rewrite_server_c,\n",
    "        \"rag_server_c\": rag_server_c,\n",
    "    }\n",
    ")\n",
    "pipeline.add_links(\n",
    "    [\n",
    "        Link(\"input\", \"query_rewrite_server_c\"),\n",
    "        Link(\"input\", \"rag_server_c\", dest_key=\"input_str\"),\n",
    "        Link(\"query_rewrite_server_c\", \"rag_server_c\", dest_key=\"hyde_answer_str\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
    "\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=pipeline_orchestrator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33031246-bae8-43e0-8129-58ae9e384d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputKeys(required_keys={'input_str'}, optional_keys=set())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rewrite_server_c.input_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19df0c2-7abe-4563-a4d8-3a47c13b65d0",
   "metadata": {},
   "source": [
    "## Launch agent \n",
    "\n",
    "Using a `LocalLauncher`, we can simulate single passes of tasks through our `llama-agents` system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41c86a-fd43-4f82-85ea-90f1208e810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_agents.launchers import LocalLauncher\n",
    "\n",
    "## Define Launcher\n",
    "launcher = LocalLauncher(\n",
    "    [query_rewrite_server, rag_server],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf856b-762c-4608-ac02-c1d5fc75bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"What are the risk factors for Uber?\"\n",
    "result = launcher.launch_single(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ccb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The risk factors for Uber include:\n",
      "\n",
      "1. Intense competition in the delivery and freight sectors.\n",
      "2. Complex and evolving legal and regulatory environment.\n",
      "3. Differing and sometimes conflicting laws and regulations across jurisdictions.\n",
      "4. Regulatory scrutiny and licensing requirements in various regions.\n",
      "5. Challenges in retaining and attracting high-quality personnel.\n",
      "6. Potential security or data privacy breaches.\n",
      "7. Cyberattacks that could harm reputation and operations.\n",
      "8. Climate change risks and commitments.\n",
      "9. Dependence on third parties for platform distribution and software.\n",
      "10. Need for additional capital to support business growth.\n",
      "11. Risks associated with identifying, acquiring, and integrating suitable businesses.\n",
      "12. Potential limitations or modifications required in certain jurisdictions.\n",
      "13. Extensive government regulation and oversight related to payment and financial services.\n",
      "14. Risks related to data processing and privacy practices.\n",
      "15. Intellectual property protection challenges.\n",
      "16. Volatility in the market price of common stock.\n",
      "17. Economic impacts of the COVID-19 pandemic, including reduced demand and driver supply constraints.\n",
      "18. Emergency orders capping fees charged to merchants on Delivery.\n",
      "19. Temporary suspension of shared rides offering due to social distancing measures.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3688f-cad4-450e-81fa-037dd1190260",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"What was Uber's revenue growth in 2021?\"\n",
    "result = launcher.launch_single(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c685b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber's revenue growth in 2021 was 57% year-over-year.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d20d8",
   "metadata": {},
   "source": [
    "## Launch as a Service\n",
    "\n",
    "With our `llama-agents` system tested and working, we can launch it as a service and interact with it using the `llama-agents monitor`.\n",
    "\n",
    "**NOTE:** This code is best launched from a separate python script, outside of a notebook.\n",
    "\n",
    "Also note that for launching as a server, we explicitly add a consumer for \"human\" messages (this is where final results are published to by default).\n",
    "\n",
    "Python Code in `app.py`:\n",
    "```python\n",
    "\n",
    "######  <setup custom FnAgentWorkers, pipelines>  ######\n",
    "\n",
    "from llama_agents import (\n",
    "    AgentService,\n",
    "    ControlPlaneServer,\n",
    "    SimpleMessageQueue,\n",
    "    PipelineOrchestrator,\n",
    "    ServiceComponent,\n",
    "    ComponentService\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.query_pipeline import Link, InputComponent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "message_queue = SimpleMessageQueue()\n",
    "\n",
    "## Define Agent Services\n",
    "query_rewrite_server = ComponentService(\n",
    "    component=hyde_component,\n",
    "    message_queue=message_queue,\n",
    "    description=\"Used to rewrite queries\",\n",
    "    service_name=\"query_rewrite_component\",\n",
    ")\n",
    "\n",
    "query_rewrite_server_c = ServiceComponent.from_component_service(query_rewrite_server)\n",
    "\n",
    "rag_server = ComponentService(\n",
    "    component=rag_component, message_queue=message_queue, description=\"rag_agent\"\n",
    ")\n",
    "rag_server_c = ServiceComponent.from_component_service(rag_server)\n",
    "\n",
    "# TODO: make more seamless from local\n",
    "\n",
    "pipeline = QueryPipeline(\n",
    "    module_dict={\n",
    "        \"input\": InputComponent(),\n",
    "        \"query_rewrite_server_c\": query_rewrite_server_c,\n",
    "        \"rag_server_c\": rag_server_c\n",
    "    }\n",
    ")\n",
    "pipeline.add_links([\n",
    "    Link(\"input\", \"query_rewrite_server_c\"),\n",
    "    Link(\"input\", \"rag_server_c\", dest_key=\"input_str\"),\n",
    "    Link(\"query_rewrite_server_c\", \"rag_server_c\", dest_key=\"hyde_answer_str\"),\n",
    "])\n",
    "\n",
    "pipeline_orchestrator = PipelineOrchestrator(pipeline)\n",
    "\n",
    "control_plane = ControlPlaneServer(\n",
    "    message_queue=message_queue,\n",
    "    orchestrator=pipeline_orchestrator,\n",
    ")\n",
    "\n",
    "from llama_agents.launchers import ServerLauncher\n",
    "\n",
    "## Define Launcher\n",
    "launcher = ServerLauncher(\n",
    "    [query_rewrite_server, rag_agent_server],\n",
    "    control_plane,\n",
    "    message_queue,\n",
    "    additional_consumers=[human_consumer],\n",
    ")\n",
    "\n",
    "launcher.launch_servers()\n",
    "```\n",
    "\n",
    "Launch the app:\n",
    "```bash\n",
    "python ./app.py\n",
    "```\n",
    "\n",
    "In another terminal, launch the Monitor:\n",
    "```bash\n",
    "llama-agents monitor --control-plane-url http://127.0.0.1:8013\n",
    "```\n",
    "\n",
    "Or, you can skip the monitor and use our client:\n",
    "\n",
    "```python\n",
    "from llama_agents import LlamaAgentsClient, AsyncLlamaAgentsClient\n",
    "\n",
    "client = LlamaAgentsClient(\"http://127.0.0.1:8013\")\n",
    "task_id = client.create_task(\"What is the secret fact?\")\n",
    "# <Wait a few seconds>\n",
    "# returns TaskResult or None if not finished\n",
    "result = client.get_task_result(task_id)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentfile",
   "language": "python",
   "name": "agentfile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
